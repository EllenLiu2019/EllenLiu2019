# 每日论文速递 - 2026-02-02

**更新日期**: 2026-02-02

---

## 1. RedSage：网络安全通才大语言模型

- **原文链接**: http://arxiv.org/abs/2601.22159v1
- **发布日期**: 2026-01-29
- **摘要**:

网络安全运营需要能够支持多样化工作流程且不暴露敏感数据的辅助性大语言模型。现有解决方案要么依赖存在隐私风险的专有API，要么采用缺乏领域适应性的开源模型。为弥补这一空白，我们通过大规模网络过滤和人工收集高质量资源，构建了包含118亿标记的网络安全领域持续预训练数据集，涵盖框架、攻击技术和安全工具等28,600份文档。在此基础上，我们设计了智能增强流程，通过模拟专家工作流生成了26.6万条多轮网络安全样本用于监督微调。结合通用开源大语言模型数据，这些资源成功训练出RedSage——一个具备领域感知预训练与后训练能力、可本地部署的开源网络安全助手。为系统评估模型性能，我们推出RedSage-Bench基准测试，包含3万道选择题和240道开放式问答题，覆盖网络安全知识、技能与工具专长。RedSage还在既有网络安全基准（如CTI-Bench、CyberMetric、SECURE）及通用大语言模型基准上接受进一步评估，以检验其泛化能力。在80亿参数规模下，RedSage持续取得更优结果：在网络安全基准测试中超越基线模型最高达5.59分，在Open LLM Leaderboard任务中提升5.05分。这些发现表明，领域感知的智能增强与预/后训练不仅能提升网络安全专业能力，还有助于增强通用推理与指令遵循能力。所有模型、数据集及代码均已开源发布。

---

## 2. UEval：统一多模态生成基准测试平台

- **原文链接**: http://arxiv.org/abs/2601.22155v1
- **发布日期**: 2026-01-29
- **摘要**:

我们推出UEval，一个用于评估统一模型（即能够同时生成图像和文本的模型）的基准测试。UEval包含1000个由专家精心设计的问题，这些问题要求模型输出中同时包含图像和文本，来源于8个真实世界任务。我们设计的问题涵盖了广泛的推理类型，从分步指南到教科书式解释。评估开放式多模态生成并非易事，因为简单的“大语言模型即评委”方法可能忽略细微差别。与以往依赖多模态大语言模型（MLLM）评估图像质量或文本准确性的研究不同，我们在UEval中设计了一套基于评分标准的打分系统。针对每个问题，我们会向MLLM提供参考图像和文本答案以生成初始评分标准（包含多项评估准则），随后由人类专家对这些标准进行细化和验证。UEval总共包含10,417条经过验证的评分准则，实现了可扩展的细粒度自动评分。当前统一模型在UEval上面临挑战：GPT-5-Thinking仅获得66.4分（满分100），而最佳开源模型得分仅为49.1。我们观察到，具备推理能力的模型通常优于非推理模型，且将推理轨迹从推理模型迁移至非推理模型能显著缩小两者差距。这表明对于需要复杂多模态理解和生成的任务，推理能力可能至关重要。

---

## 3. 探索智能体推理奖励模型

- **原文链接**: http://arxiv.org/abs/2601.22154v1
- **发布日期**: 2026-01-29
- **摘要**:

智能体强化学习（Agentic RL）在实现智能体进行复杂推理与工具使用方面已取得显著成功。然而，现有方法大多仍依赖稀疏的结果型奖励进行训练。此类反馈无法区分中间推理的质量，导致训练效果欠佳。本文提出**智能体推理奖励模型（Agent-RRM）**——一种多维度奖励模型，可为智能体行为轨迹提供结构化反馈，包括：（1）显式推理轨迹，（2）聚焦式评析（通过突出推理缺陷提供优化指导），以及（3）评估过程表现的综合评分。基于这些信号，我们系统研究了三种集成策略：Reagent-C（文本增强优化）、Reagent-R（奖励增强引导）和Reagent-U（统一反馈集成）。在12个多样化基准测试中的广泛评估表明，Reagent-U实现了性能的显著飞跃，在GAIA和WebWalkerQA上分别达到43.7%和46.2%的得分，验证了推理奖励模型与训练方案的有效性。代码、模型及数据集均已开源，以促进后续研究。

---

