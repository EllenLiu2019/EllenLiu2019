# 每日论文速递 - 2025-12-10

**更新日期**: 2025-12-10

---

## 1. 泛化结果是否具有普适性？

- **原文链接**: http://arxiv.org/abs/2512.07832v1
- **发布日期**: 2025-12-08
- **摘要**:

大型语言模型（LLM）的分布外泛化能力对其实际部署至关重要。然而，现有评估LLM泛化性能的研究通常仅关注单一分布外数据集。这种方法可能无法精确评估模型能力，因为模型部署后遇到的数据偏移远比实验设定更为多样。本研究旨在探究分布外泛化结果是否具有可推广性。具体而言，我们在微调过程中评估模型在多个分布外测试集上的表现，并通过回归剔除领域内性能的影响，计算这些测试集性能之间的偏相关性。这种方法使我们能够评估在控制领域内性能后，不同泛化表现之间的关联程度。通过对OLMo2和OPT模型的分析，我们发现泛化结果并不存在统一规律：任意两个分布外测试集之间呈现正相关或负相关性，很大程度上取决于所分析的具体模型选择。

---

## 2. 协同因果感知：弥合人机决策支持中的互补性鸿沟

- **原文链接**: http://arxiv.org/abs/2512.07801v1
- **发布日期**: 2025-12-08
- **摘要**:

基于大语言模型的智能体正迅速融入专家决策支持系统，然而在复杂多变的高风险环境中，它们鲜少能真正提升团队智慧：人机协作团队的表现往往逊色于最优个体，专家们常在验证循环与过度依赖间摇摆不定，承诺的互补效应也未能实现。我们认为这不仅是准确性问题，更暴露出当前AI辅助理念的根本缺陷：专家决策本质上是协作性认知过程，人类与AI需要持续共同构建、检验并修正彼此的心智模型、目标与约束条件。为此，我们提出"协作因果意义构建"作为决策支持智能体的研究纲领与组织框架：这类系统应设计为认知工作的合作伙伴，能够持续学习特定专家的推理模式，协助阐明并修正目标，共同构建并压力测试因果假设，并从联合决策结果中持续学习，实现人类与智能体的共同进化。我们勾勒出三大挑战方向：构建使协作思考具有工具价值的训练生态、开发支持协同建模的表征与交互协议、建立以信任与互补性为核心的评价体系。这些研究方向可将多智能体系统研究重新聚焦于参与协作意义构建的智能体，使其成为能与人类伙伴共同思考的AI队友。

---

## 3. ReasonBENCH：评估大语言模型推理的（不）稳定性

- **原文链接**: http://arxiv.org/abs/2512.07795v1
- **发布日期**: 2025-12-08
- **摘要**:

大型语言模型（LLM）正越来越多地部署在需要推理能力的场景中，例如多步骤问题求解和思维链任务。然而，当前的评估实践普遍仅报告单次运行的准确率，而忽略了随机解码过程中自然产生的内在不确定性。这种忽略造成了认知盲区，因为实践者无法可靠评估某方法所报告的性能是否稳定、可复现或成本一致。为此，我们推出了首个旨在量化LLM推理底层不稳定性的基准测试——ReasonBENCH。该基准提供：（一）模块化评估库，用于标准化推理框架、模型与任务；（二）多轮运行协议，可报告质量和成本两方面具有统计可靠性的指标；（三）公开排行榜，以鼓励关注方差的评估报告。通过对不同领域任务的测试，我们发现绝大多数推理策略和模型均表现出高度不稳定性。值得注意的是，即使平均性能相近的策略，其置信区间宽度可能相差四倍之多，且性能最优的方法往往伴随着更高且更不稳定的成本。这种不稳定性损害了跨运行实验的可复现性，进而影响所报告性能的可靠性。为深入理解这一现象，我们进一步分析了提示设计、模型家族和规模对解题成功率与稳定性之间权衡关系的影响。我们的研究结果凸显了可复现性作为可靠LLM推理的关键维度，并为未来推理方法与不确定性量化技术奠定了基础。ReasonBENCH已公开于https://github.com/au-clan/ReasonBench。

---

