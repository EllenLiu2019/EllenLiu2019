# 每日论文速递 - 2026-01-14

**更新日期**: 2026-01-14

---

## 1. 信心陷阱：大型语言模型中的性别偏见与预测确定性

- **原文链接**: http://arxiv.org/abs/2601.07806v1
- **发布日期**: 2026-01-12
- **摘要**:

随着大型语言模型在敏感领域的应用日益增多，其置信度分数与公平性及偏见的关联性引发广泛关注。本研究探讨了LLM预测置信度与人工标注偏见判断之间的对应关系。研究聚焦于性别偏见，重点考察涉及性别化代词消解场景中的概率置信度校准问题，旨在评估基于预测置信度的校准指标能否有效捕捉LLM中与公平性相关的差异。实验结果显示，在六种前沿模型中，Gemma-2在性别偏见基准测试中表现出最差的校准性能。本研究的核心贡献在于提出了面向公平性的LLM置信度校准评估框架，为伦理部署提供指引。此外，我们创新性地提出了"性别均衡校准误差"指标，专门用于衡量消解任务中的性别差异程度。

---

## 2. 对话中学习：解析人类与大型语言模型在政治议题上的互动动态

- **原文链接**: http://arxiv.org/abs/2601.07796v1
- **发布日期**: 2026-01-12
- **摘要**:

大型语言模型(LLM)作为学习对话伙伴的应用日益广泛，但其支持用户学习与参与度的交互机制尚未得到充分研究。我们通过分析397场关于社会政治议题的人机对话中LLM与参与者的语言及交互特征，揭示了LLM解释行为影响政治知识水平与信心变化的机制与条件。中介分析表明：LLM解释的丰富性通过促进用户反思性洞察，部分支撑了信心提升；而其促进知识获取的作用则完全通过用户认知投入实现。调节分析显示：这些效应具有高度条件性，并随政治效能感差异而变化——信心提升取决于高效能感用户如何体验并化解不确定性；知识增长则依赖于高效能感用户能否利用延伸互动，较长的对话主要使善于反思的用户受益。总而言之，我们发现从LLM中学习是一种交互性成就，而非优质解释的必然结果。研究结果强调，在设计人机交互系统时，必须使LLM的解释行为与用户参与状态相协调，方能有效支持学习过程。

---

## 3. 多跳推理的亲属关系数据基准

- **原文链接**: http://arxiv.org/abs/2601.07794v1
- **发布日期**: 2026-01-12
- **摘要**:

大型语言模型（LLM）在多跳推理能力——即整合多条信息形成连贯推断——方面的评估日益受到关注。我们提出KinshipQA基准测试，旨在通过亲属关系推理来探究这一能力。本研究的核心贡献在于开发了一种生成式流程，能够按需生成大规模、高真实性且具有文化特异性的谱系数据：这些相互关联的家族树集合满足不同亲属制度下明确的婚姻约束条件。该流程使得任务难度、文化预设和关系深度得以被系统化控制和调整。基于这些谱系数据，我们构建了需要通过对隐含关系链进行推理的文本推断任务。我们采用统一的零样本协议与确定性解码策略，对六种涵盖开源与闭源模型的最先进LLM进行了评估，并通过精确匹配和基于集合的指标衡量性能。实验结果表明，KinshipQA能产生差异显著的结果，并揭示不同模型与文化场景下多跳推理能力的系统性差异。

---

