# 每日论文速递 - 2026-02-06

**更新日期**: 2026-02-06

---

## 1. 强化注意力学习

- **原文链接**: http://arxiv.org/abs/2602.04884v1
- **发布日期**: 2026-02-04
- **摘要**:

通过测试时扩展，强化学习（RL）的后训练已显著提升了大语言模型（LLM）的推理能力。然而，在多模态大语言模型（MLLM）中沿用这一范式，通过冗长的推理过程进行优化，对感知能力的提升效果有限，甚至可能导致性能下降。我们提出了强化注意力学习（RAL），这是一种策略梯度框架，直接优化内部注意力分布而非输出词元序列。通过将优化目标从"生成什么"转向"关注何处"，RAL促进了复杂多模态输入中的有效信息分配与更精准的语义落地。在多样化图像与视频基准测试中的实验表明，该方法相比GRPO及其他基线模型均取得稳定提升。我们进一步提出同策略注意力蒸馏技术，证明迁移潜在注意力行为比标准知识蒸馏能实现更强的跨模态对齐效果。这些研究成果将注意力策略确立为多模态后训练中一种原理清晰且通用的替代方案。

---

## 2. 重新审视大语言模型强化学习中的信任区域

- **原文链接**: http://arxiv.org/abs/2602.04879v1
- **发布日期**: 2026-02-04
- **摘要**:

强化学习已成为微调大语言模型的核心方法，其中近端策略优化算法被视为实际标准。尽管该算法应用广泛，我们认为PPO核心的概率比截断机制在结构上并不适合大语言模型固有的巨大词汇量特性。PPO基于采样词元的概率比对策略更新进行约束，这种概率比本质上是真实策略散度的噪声单样本蒙特卡洛估计。这种机制形成了次优的学习动态：低频词元的更新会遭受过度惩罚，而高频词元可能出现的灾难性偏移却约束不足，导致训练效率低下且不稳定。

为解决这一问题，我们提出散度近端策略优化算法，用基于策略散度直接估计（如全变差或KL散度）的原理性约束替代启发式截断。为避免巨大的内存占用，我们引入高效的二元与Top-K近似方法，以可忽略的开销捕捉核心散度信息。大量实验评估表明，相较于现有方法，DPPO在训练稳定性和效率上表现更优，为基于强化学习的大语言模型微调提供了更稳健的基础。

---

## 3. 数据中的潜意识效应：一种基于对数线性的通用机制

- **原文链接**: http://arxiv.org/abs/2602.04863v1
- **发布日期**: 2026-02-04
- **摘要**:

训练现代大型语言模型（LLM）已成为算法与数据集的丰盛拼盘，这些设计旨在激发特定行为，因此开发理解数据集对模型特性影响的技术至关重要。近期实验进一步凸显了这一挑战：研究表明数据集能够传递无法从单个数据点直接观测到的信号，这对以数据集为中心的LLM训练理解提出了概念性挑战，并暗示此类现象缺乏根本性解释。为探究这类效应，受近期关于LLM线性结构研究的启发，我们揭示了一种通用机制，可解释隐藏潜文本如何在泛化数据集中产生。

我们提出**对数线性选择法**（LLS），该方法能指导如何从通用偏好数据集中选择子集，以激发广泛的隐藏效应。通过应用LLS，我们在真实世界数据集中发现了特定子集，使得基于这些子集训练的模型展现出多样行为：从持有特定偏好，到以数据集中未出现的外语响应提示，再到呈现不同人格特征。关键的是，这种效应在被选子集中持续存在，且跨越不同架构的模型，证明了其普适性与通用性。

---

