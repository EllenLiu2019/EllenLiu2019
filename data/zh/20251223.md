# 每日论文速递 - 2025-12-23

**更新日期**: 2025-12-23

---

## 1. ShareChat：一个真实环境下的聊天机器人对话数据集

- **原文链接**: http://arxiv.org/abs/2512.17843v1
- **发布日期**: 2025-12-19
- **摘要**:

尽管大型语言模型（LLM）已发展为具有独特界面设计和功能差异的独立平台，但现有公共数据集仍将其视为通用文本生成器，剥离了直接影响用户交互的界面语境。为弥补这一缺陷，我们推出ShareChat——一个跨平台的大规模语料库，包含从ChatGPT、Claude、Gemini、Perplexity和Grok五大平台公开分享的URL中收集的142,808段对话及超66万轮交互。ShareChat的独特性在于保留了标准日志中常丢失的原生平台功能痕迹，包括推理过程、来源链接和代码片段，同时涵盖2023年4月至2025年10月期间的101种语言。此外，该数据集相比以往研究提供了更长的上下文窗口和更深的交互层级。我们通过三项代表性分析展示了数据集的多维应用价值：（1）通过对话完整性分析衡量用户意图满足度；（2）评估内容生成中的来源引用行为；（3）开展时序分析追踪使用模式的演变趋势。这项工作为学术界理解真实场景中用户与LLM聊天机器人的交互提供了重要且及时的研究资源。

---

## 2. DEER：一个全面可靠的深度研究专家报告基准

- **原文链接**: http://arxiv.org/abs/2512.17776v1
- **发布日期**: 2025-12-19
- **摘要**:

随着大语言模型（LLM）的快速发展，深度研究系统已能通过多步推理与证据合成生成专家级报告，但对此类报告的评估仍面临挑战。现有基准常缺乏系统性专家报告评价标准，依赖大语言模型作为评判者的评估方法难以捕捉需要专业判断的问题，且来源验证通常仅覆盖明确引用的有限陈述，而非报告整体的真实性可靠性。为此，我们推出DEER基准——专为评估专家级深度研究报告而设计。该基准包含横跨13个领域的50项报告撰写任务，以及一套基于专家知识构建的评估体系（7个维度、25个子维度），细化为130项精细化评分标准。DEER进一步提供任务专属的专家指导，帮助大语言模型评判者更一致地评估专家级报告质量。为补充基于量规的评估，我们提出文档级事实核查架构，可提取并验证报告中所有主张（包括引用与未引用内容），并量化外部证据质量。DEER基准与人类专家判断高度吻合，并能生成可解释的系统优势与薄弱环节诊断报告。

---

## 3. 当黄金标准未必标准：评估用户生成内容翻译的挑战

- **原文链接**: http://arxiv.org/abs/2512.17738v1
- **发布日期**: 2025-12-19
- **摘要**:

用户生成内容（UGC）的特点是频繁使用非标准语言，从拼写错误到表情达意的选择，如俚语、字符重复和表情符号。这使得评估UGC翻译尤为困难："好"翻译的标准取决于输出所需的标准程度。为探究此问题，我们分析了四个UGC数据集的人工翻译指南，并归纳出十二种非标准现象和五种翻译操作（规范化、复制、转移、省略、审查）。分析显示，不同数据集处理UGC的方式存在显著差异，导致参考译文呈现出标准程度的连续光谱。通过对大语言模型（LLMs）的案例研究，我们发现翻译评分对包含明确UGC翻译指令的提示极为敏感，当这些指令与数据集指南一致时，评分会有所提升。我们认为，在需要保留UGC风格的情况下，公平评估要求模型和指标都能理解翻译指南。最后，我们呼吁在数据集创建过程中制定清晰指南，并开发可控、能感知指南的UGC翻译评估框架。

---

