# 每日论文速递 - 2025-12-22

**更新日期**: 2025-12-22

---

## 1. 生成对抗推理器：通过对抗性强化学习增强大语言模型推理能力

- **原文链接**: http://arxiv.org/abs/2512.16917v1
- **发布日期**: 2025-12-18
- **摘要**:

具备显式推理能力的大语言模型在数学推理方面表现出色，但仍存在过程性错误，例如计算失误、逻辑脆弱以及表面合理但实际无效的推理步骤。本文提出生成对抗推理器——一种基于策略的联合训练框架，通过对抗性强化学习协同进化大语言模型推理器与基于大语言模型的判别器，从而提升推理能力。该框架采用计算高效的审查机制，将每条推理链划分为逻辑完整且长度相近的片段，判别器通过简洁的结构化论证评估每个片段的严谨性。学习过程融合了互补信号：大语言模型推理器因生成逻辑一致且能得出正确答案的步骤而获得奖励，判别器则通过准确检测错误或区分推理过程中的痕迹来获取奖励。这种机制产生了密集、校准良好、基于策略的步骤级奖励，补充了稀疏的精确匹配信号，从而改善了信用分配、提升了样本效率，并增强了大语言模型的整体推理质量。在多个数学基准测试中，该方法相较于采用标准强化学习后训练的强基线模型均取得稳定提升。具体而言，在AIME24测试中，我们将DeepSeek-R1-Distill-Qwen-7B的得分从54.0提升至61.3（+7.3），将DeepSeek-R1-Distill-Llama-8B的得分从43.7提升至53.7（+10.0）。模块化判别器还能灵活调整奖励机制，以适应教师蒸馏、偏好对齐及基于数学证明的推理等目标。

---

## 2. 建设性电路放大：通过针对性子网络更新提升大语言模型的数学推理能力

- **原文链接**: http://arxiv.org/abs/2512.16914v1
- **发布日期**: 2025-12-18
- **摘要**:

先前关于大型语言模型内部工作机制的研究发现，存在被称为"电路"的稀疏子网络负责执行特定任务。此外，研究表明通过微调提升模型性能往往源于对模型中已有电路的强化。综合这些发现，我们推测直接干预此类电路以实现精准、任务导向的更新具有可行性。受此启发，我们提出了一种名为"建构式电路增强"的新方法：通过分析模型推理轨迹识别关键标记，定位负责目标任务的模型组件，并仅对这些组件进行更新。在数学推理任务中应用该方法，仅需修改1.59%的模型组件，即可在多个模型上实现最高11.4%的准确率提升，同时经MMLU、TriviaQA和TruthfulQA基准测试验证，对其他能力的影响微乎其微。这些结果表明，通过选择性更新少量模型组件，能够可靠地增强模型的定向能力。

---

## 3. 探索与利用之辩：通过剪裁、熵与伪奖励重新审视RLVR

- **原文链接**: http://arxiv.org/abs/2512.16912v1
- **发布日期**: 2025-12-18
- **摘要**:

本文研究了具有可验证奖励的强化学习（RLVR）中的探索-利用权衡问题，该框架旨在提升大语言模型（LLMs）的推理能力。近期研究表明，RLVR可通过两种看似矛盾的机制激发LLMs强大的数学推理能力：一是伪奖励机制，即通过奖励与真实答案无关的结果来抑制模型对已知策略的过度利用；二是熵最小化机制，即推动模型输出更自信、更确定的结果以抑制探索行为。这两种机制揭示了一个令人困惑的动态现象：抑制利用与抑制探索均能提升推理性能，但其背后协调这两种效应的原理尚不明确。本文聚焦两个核心问题：（i）策略熵如何影响性能；（ii）伪奖励是否通过裁剪偏差与模型污染的相互作用产生增益。实验结果表明，在伪奖励作用下，裁剪偏差会降低策略熵，促使模型输出更自信且确定的结果，而仅靠熵最小化并不足以实现性能提升。我们进一步提出奖励错配模型，以解释为何伪奖励能在非污染场景下提升性能。本研究阐明了伪奖励产生增益的内在机制，并为更有效的RLVR训练提供了理论依据。

---

