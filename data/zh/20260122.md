# 每日论文速递 - 2026-01-22

**更新日期**: 2026-01-22

---

## 1. 哪种推理轨迹能更好地教会学生推理？一种简单的信息对齐度量方法

- **原文链接**: http://arxiv.org/abs/2601.14249v1
- **发布日期**: 2026-01-20
- **摘要**:

长链思维轨迹为从教师大语言模型向学生模型提炼推理能力提供了丰富的监督信号。然而，先前研究及我们的实验均表明，来自更强教师的轨迹未必能培养出更优秀的学生模型，这凸显了数据与学生模型适配性在知识蒸馏中的重要性。现有方法主要通过学生模型的似然度评估适配性，倾向于选择与模型当前行为高度一致的轨迹，却忽略了信息量更丰富的样本。针对这一问题，我们提出**排序-惊异比**这一简洁指标，它能同时捕捉对齐度和信息量，从而评估推理轨迹的适配性。该指标的提出基于以下观察：有效轨迹通常具备双重特征——在学生模型下既呈现较低的绝对概率，又对应相对高排名的词元，从而在信号强度与行为对齐间取得平衡。具体而言，RSR定义为轨迹在词元级别平均排序与平均负对数似然之比，其计算与解释均直观简便。在涵盖五个学生模型和来自11位不同教师推理轨迹的实验中，RSR与训练后性能表现出强相关性（平均斯皮尔曼系数达0.86），显著优于现有评估指标。我们进一步展示了该指标在轨迹筛选和教师选择两个实际场景中的应用价值。

---

## 2. Jet-RL：通过统一训练与部署精度流程实现基于策略的FP8强化学习

- **原文链接**: http://arxiv.org/abs/2601.14243v1
- **发布日期**: 2026-01-20
- **摘要**:

强化学习（RL）对于提升大语言模型（LLMs）的复杂推理能力至关重要。然而，现有的RL训练流程存在计算效率低、资源消耗大的问题，其中推演阶段占用了总训练时间的70%以上。量化RL训练，尤其是采用FP8精度，为解决这一瓶颈提供了可行路径。当前普遍采用的策略是在推演阶段使用FP8精度，同时在训练阶段保留BF16精度。本研究首次对FP8 RL训练进行了全面分析，并证明这种广泛使用的"BF16训练+FP8推演"策略在长序列推演和复杂任务场景下存在严重的训练不稳定性和灾难性精度崩溃问题。分析表明，这些失效源于该方法的离策略特性，导致训练与推理之间存在显著的数值失配。

基于这些发现，我们提出了Jet-RL——一个能够实现稳健稳定RL优化的FP8训练框架。其核心思想是采用统一的FP8精度流同时处理训练和推演，从而最大程度减少数值差异，并消除低效的跨步骤校准需求。大量实验验证了Jet-RL的有效性：相较于BF16训练，我们的方法在推演阶段实现了最高33%的加速，训练阶段最高41%的加速，端到端整体加速达到16%，同时在所有实验设置中保持稳定收敛，且精度损失可忽略不计。

---

## 3. APEX智能体

- **原文链接**: http://arxiv.org/abs/2601.14242v1
- **发布日期**: 2026-01-20
- **摘要**:

我们推出面向智能体的AI生产力指数（APEX-Agents），这是一个用于评估AI智能体能否执行由投行分析师、管理顾问和企业律师设计的跨应用长周期任务的基准测试体系。APEX-Agents要求智能体在包含真实文件与工具的工作环境中进行任务导航。我们采用Pass@1指标对八个智能体进行排行榜测试，其中Gemini 3 Flash（思考模式=深度）以24.0%的得分位居榜首，紧随其后的是GPT-5.2（思考模式=深度）、Claude Opus 4.5（思考模式=深度）和Gemini 3 Pro（思考模式=深度）。现开源APEX-Agents基准测试集（样本量=480），包含全部提示模板、评估标准、标准答案、文件及元数据。同时开源智能体执行与评估基础设施Archipelago。

---

