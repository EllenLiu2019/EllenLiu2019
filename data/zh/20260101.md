# 每日论文速递 - 2026-01-01

**更新日期**: 2026-01-01

---

## 1. 在多轮对话中引导行为

- **原文链接**: http://arxiv.org/abs/2512.23701v1
- **发布日期**: 2025-12-29
- **摘要**:

在对话环境中识别大型语言模型（LLM）的特定且通常复杂的行为，对其评估至关重要。近期研究提出了新颖技术，旨在寻找能够诱导目标模型产生特定行为的自然语言提示，但这些方法主要针对单轮对话场景。本研究聚焦于多轮对话背景下的行为诱导问题。我们首先提出一个分析框架，将现有方法根据其与目标模型的交互方式分为三类：仅依赖先验知识的方法、基于离线交互的方法，以及从在线交互中学习的方法。随后，我们引入一种广义的多轮在线方法框架，统一了单轮与多轮行为诱导范式。我们通过自动生成多轮测试用例对三类方法进行评估，并通过分析查询预算（即与目标模型的交互次数）与成功率（即成功诱导行为的输入发现率）之间的权衡关系，探究这些方法的效率。研究发现，在三个任务中，在线方法仅需数千次查询即可实现平均45%/19%/77%的成功率，而现有多轮对话基准中的静态方法仅能发现极少甚至无法发现失效案例。本研究揭示了行为诱导方法在多轮对话评估中的创新应用，并强调了研究社区向动态基准测试迈进的重要性。

---

## 2. 基于文本细粒度人工反馈的LLMs微调

- **原文链接**: http://arxiv.org/abs/2512.23693v1
- **发布日期**: 2025-12-29
- **摘要**:

我们提出了一种基于反馈驱动改进链的偏好监督微调方法及相应数据集。给定模型生成的响应，标注者通过标记"喜欢"和"不喜欢"的文本片段并提供具体理由，从而提供细粒度反馈。基础模型随后从左到右依次重写被标记为不喜欢的片段，形成渐进式改进序列。我们通过链中相邻步骤构建直接对齐的偏好配对，使模型能够从局部化、有针对性的编辑中学习。实验表明，该方法在性能上超越了基于标准A/B偏好排序或完整对比重写的直接对齐方法，证明这种结构化、基于修订的监督机制能实现更高效、更有效的偏好调优。

---

## 3. 基于大语言模型的学术评审中的多语言隐藏提示注入攻击

- **原文链接**: http://arxiv.org/abs/2512.23684v1
- **发布日期**: 2025-12-29
- **摘要**:

大型语言模型（LLM）正越来越多地被考虑应用于高影响力工作流程，包括学术同行评审。然而，LLM容易受到文档级隐藏提示注入攻击的影响。在本研究中，我们构建了一个包含约500篇被ICML收录的真实学术论文的数据集，并评估了在这些文档中嵌入隐藏对抗性提示的效果。每篇论文被注入了四种不同语言但语义相同的指令，并使用LLM进行评审。我们发现，英语、日语和中文的提示注入会导致评审分数和录用/拒绝决定发生显著变化，而阿拉伯语注入几乎不产生任何影响。这些结果凸显了基于LLM的评审系统对文档级提示注入的脆弱性，并揭示了不同语言间漏洞的显著差异。

---

