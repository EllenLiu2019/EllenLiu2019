# 每日论文速递 - 2026-01-10

**更新日期**: 2026-01-10

---

## 1. LELA：一种基于大语言模型的实体链接方法，具备零样本领域自适应能力

- **原文链接**: http://arxiv.org/abs/2601.05192v1
- **发布日期**: 2026-01-08
- **摘要**:

实体链接（将文本中的歧义指称映射到知识库中的实体）是知识图谱构建、问答系统及信息抽取等任务的基础步骤。我们提出的LELA方法采用模块化由粗到精的处理流程，充分利用大语言模型的能力，无需任何微调阶段即可适配不同目标领域、知识库及大语言模型。在多种实体链接场景下的实验表明，LELA与经过微调的方法相比具有高度竞争力，且显著优于未经微调的基线方法。

---

## 2. 大语言模型在自我消耗式表演循环中的偏见观察与矫正策略

- **原文链接**: http://arxiv.org/abs/2601.05184v1
- **发布日期**: 2026-01-08
- **摘要**:

大型语言模型（LLM）的快速发展，使得利用合成数据训练未来模型的研究日益受到关注。然而，这形成了一个自我消耗的再训练循环——模型基于自身输出进行训练，可能导致性能下降并诱发新兴偏见。在实际应用中，先前部署的LLM可能影响其生成的数据，从而形成由用户反馈驱动的动态系统。例如，若模型持续对某一用户群体服务不足，从该特定人口统计群体收集的查询数据将会减少。本研究提出**自我消耗表现循环（SCPL）**的概念，并在受控表现反馈条件下，探究合成数据在这些动态迭代训练过程中对偏见形成的作用。这一受控实验设置的动机在于，动态生产系统中的真实用户偏好数据难以获取，使我们能够以系统化的方式隔离和分析反馈驱动的偏见演化。我们重点关注两种循环类型：典型的再训练设置与尚未充分探索的增量微调设置。通过在三个实际任务上的实验，我们发现表现循环会增强偏好偏见，同时降低差异偏见。为此，我们设计了一种基于奖励的拒绝采样策略来缓解偏见，推动自改进系统向更可信的方向发展。

---

## 3. 由内而外：构建面向用户的核心记忆树演进机制，实现长期个性化对话系统

- **原文链接**: http://arxiv.org/abs/2601.05171v1
- **发布日期**: 2026-01-08
- **摘要**:

现有长期个性化对话系统难以在无限交互流与有限上下文约束之间取得平衡，常受困于记忆噪声累积、推理能力退化及人设一致性缺失等问题。为应对这些挑战，本文提出"由内向外"（Inside Out）框架，以全局维护的PersonaTree作为长期用户画像载体。通过初始模式约束主干并动态更新枝叶，PersonaTree实现了可控生长，在保持一致性的同时完成记忆压缩。此外，我们通过基于过程的强化学习奖励机制训练轻量化记忆监听器（MemListener），使其能生成结构化、可执行、可解释的{添加、更新、删除、无操作}指令，从而支撑个性化树结构的动态演化。在响应生成阶段，PersonaTree可直接用于增强时延敏感场景的输出效果；当用户需要更多细节时，系统将触发代理模式，在PersonaTree约束下按需引入细节信息。实验表明，PersonaTree在抑制上下文噪声和保持人设一致性方面，优于全文拼接及多种个性化记忆系统。值得注意的是，小型MemListener模型在记忆操作决策性能上已达到甚至超越DeepSeek-R1-0528、Gemini-3-Pro等强大推理模型的表现。

---

