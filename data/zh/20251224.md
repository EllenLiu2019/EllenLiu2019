# 每日论文速递 - 2025-12-24

**更新日期**: 2025-12-24

---

## 1. GenEnv：LLM智能体与环境模拟器间的难度对齐协同进化

- **原文链接**: http://arxiv.org/abs/2512.19682v1
- **发布日期**: 2025-12-22
- **摘要**:

训练能力强的大型语言模型（LLM）智能体，目前严重受限于现实世界交互数据的高成本与静态特性。为解决这一问题，我们提出了GenEnv框架，该框架通过在智能体与可扩展的生成式环境模拟器之间建立难度对齐的协同进化博弈机制。与传统基于静态数据集进行模型进化的方法不同，GenEnv实现了数据动态演化：模拟器充当动态课程策略，持续生成与智能体“最近发展区”精准匹配的任务。这一过程由简洁高效的α-课程奖励机制引导，确保任务难度与智能体当前能力水平相适应。我们在API-Bank、ALFWorld、BFCL、Bamboogle和TravelPlanner五个基准测试中对GenEnv进行评估。实验表明，GenEnv将7B基线模型的智能体性能最高提升**40.3%**，其平均表现达到甚至超越了更大规模模型的水平。与基于Gemini 2.5 Pro的离线数据增强方法相比，GenEnv仅需消耗**1/3.3**的数据量即可实现更优性能。通过从静态监督转向自适应模拟，GenEnv为扩展智能体能力提供了一条高效的数据利用路径。

---

## 2. 自下而上的策略优化：你的语言模型策略中暗藏内部策略

- **原文链接**: http://arxiv.org/abs/2512.19673v1
- **发布日期**: 2025-12-22
- **摘要**:

现有的强化学习（RL）方法通常将大语言模型（LLMs）视为单一统一策略，忽略了其内部机制。因此，理解策略在不同层级和模块间的演化过程，对于实现更具针对性的优化以及揭示复杂推理机制至关重要。本文通过利用Transformer残差流的内在分割特性，以及隐藏状态与解嵌入矩阵的组合等价于可采样策略这一性质，对语言模型策略进行分解。该分解揭示了内部层级策略（对应各独立层级的贡献）和内部模块化策略（与每层中的自注意力及前馈网络组件对齐）。通过分析内部策略的熵，我们发现：（a）早期层级保持高熵以支持探索，顶层则收敛至接近零熵以进行精细化调整，且收敛模式在不同模型系列中存在差异；（b）Llama模型的预测空间在最后一层快速收敛，而Qwen系列模型（尤其是Qwen3）展现出更接近人类、逐步结构化的推理模式。基于这些发现，我们提出了一种新颖的强化学习范式——自底向上策略优化（BuPO），该方法在早期训练阶段直接优化内部层级策略。通过在较低层级对齐训练目标，BuPO重构了基础推理能力并实现了卓越性能。在复杂推理基准测试上的大量实验验证了本方法的有效性。代码已开源：https://github.com/Trae1ounG/BuPO。

---

## 3. 探索零样本情感方面分类与统一意义表示在思维链提示中的应用

- **原文链接**: http://arxiv.org/abs/2512.19651v1
- **发布日期**: 2025-12-22
- **摘要**:

方面-类别情感分析（ACSA）通过识别评论中的特定主题及其相关情感，提供细粒度的洞察。尽管监督学习方法在该领域占据主导地位，但新领域标注数据的稀缺性和高昂成本构成了显著障碍。我们认为，在数据标注资源有限的情况下，采用零样本设置的大型语言模型（LLMs）是一种可行的替代方案。本研究提出了一种新颖的思维链（CoT）提示技术，该技术利用中间统一意义表示（UMR）来构建ACSA任务的推理过程。我们在三种模型（Qwen3-4B、Qwen3-8B和Gemini-2.5-Pro）及四个多样化数据集上，将这种基于UMR的方法与标准CoT基线进行了对比评估。研究结果表明，UMR的有效性可能具有模型依赖性。虽然初步数据显示，对于Qwen3-8B等中等规模模型，其性能表现与基线相当，但这些发现仍需进一步探究，特别是在较小模型架构上的潜在适用性方面。未来研究需验证这些结论在不同模型规模间的普适性。

---

