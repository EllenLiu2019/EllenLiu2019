# 每日论文速递 - 2026-01-27

**更新日期**: 2026-01-27

---

## 1. 大语言模型在跨度标注任务中的策略

- **原文链接**: http://arxiv.org/abs/2601.16946v1
- **发布日期**: 2026-01-23
- **摘要**:

大型语言模型（LLMs）正日益广泛地应用于文本分析任务，如命名实体识别或错误检测。然而，与基于编码器的模型不同，生成式架构缺乏显式机制来指向输入文本的特定部分。这导致了多种临时性的提示策略用于跨度标注，且结果往往不一致。本文将这些策略归纳为三类：对输入文本进行标记、标注跨度的数值位置索引，以及匹配跨度内容。为弥补内容匹配方法的局限性，我们提出了LogitMatch——一种新的约束解码方法，强制模型输出与有效的输入跨度对齐。我们在四项多样化任务中对所有方法进行了评估。研究发现，尽管标记法仍是稳健的基线方法，但LogitMatch通过消除跨度匹配问题，在基于匹配的竞争方法中实现了性能提升，并在某些实验设置中优于其他策略。

---

## 2. 基于大语言模型的事实核查系统对抗性说服攻击

- **原文链接**: http://arxiv.org/abs/2601.16890v1
- **发布日期**: 2026-01-23
- **摘要**:

自动事实核查系统易受对抗性攻击影响，导致虚假声明得以规避检测。现有对抗框架通常依赖注入噪声或改变语义，但尚未有框架利用说服技术的对抗潜力——这种技术在虚假信息传播中被广泛用于操纵受众。本文通过使用生成式大语言模型，借助说服技术对声明进行改写，提出了一类新型的、针对自动事实核查系统的说服性对抗攻击。我们选取了6大类共15种说服技术，采用解耦评估策略，研究了说服技术对声明验证和证据检索的影响。在FEVER和FEVEROUS基准测试上的实验表明，说服性攻击能显著降低验证性能和证据检索效果。我们的分析证实说服技术是一类强效的对抗攻击手段，凸显了构建更鲁棒的自动事实核查系统的必要性。

---

## 3. 推理能力提升心智理论任务的鲁棒性

- **原文链接**: http://arxiv.org/abs/2601.16853v1
- **发布日期**: 2026-01-23
- **摘要**:

大型语言模型（LLM）近期在心理理论测试中展现出强劲性能，引发了关于其底层能力本质与真实表现的争论。与此同时，通过可验证奖励的强化学习训练的推理导向型LLM已在多项基准测试中取得显著进步。本文通过改良版机器心理实验与经典基准测试结果，系统考察了此类推理模型在心理理论任务中的表现。研究发现，推理模型在面对提示变化与任务扰动时始终表现出更强的鲁棒性。分析表明，这种性能提升更可能源于模型寻找正确答案的稳健性增强，而非源于全新的心理理论推理模式。本文进一步探讨了该结论对评估LLM社会认知行为的重要意义。

---

