# 每日论文速递 - 2025-12-02

**更新日期**: 2025-12-02

---

## 1. 四比六：采用自适应块缩放的更精确NVFP4量化方法

- **原文链接**: http://arxiv.org/abs/2512.02010v1
- **发布日期**: 2025-12-01
- **摘要**:

随着大语言模型规模不断扩大，低精度数值格式（如NVFP4）因其在速度和内存方面的优势日益受到青睐。然而，要利用NVFP4加速计算，所有矩阵乘法操作数——前向传播中的权重和激活值，以及反向传播中的权重、激活值和梯度——都必须量化为NVFP4格式，这往往导致训练过程中的发散现象和推理时的性能下降。NVFP4通过为每个数值块评估多个潜在缩放因子来解决此问题。针对这一挑战，本研究提出"四分之六"（4/6）改进方案，对NVFP4量化算法进行优化，为每个数值块评估两种潜在缩放因子。与整数格式不同，FP4等浮点格式在每个数值块中对接近最大值的数具有最大量化误差，我们发现这主要是导致下游性能下降的原因。研究表明，对某些数值块而言，缩放至较小的FP4值可使可表示数值的分布更均匀，从而改善接近最大值数据的表示效果。值得注意的是，4/6方案可在英伟达Blackwell GPU上高效实现，使得在NVFP4训练大语言模型时具备可行性。在基于Transformer和混合模型架构的预训练实验中，我们发现4/6方案在多种情况下能有效防止训练发散，相较于采用当前最先进NVFP4训练方案的模型，其训练损失显著更接近BF16精度。研究还表明，4/6方案可轻松融入多种后训练量化方法，普遍提升下游任务准确率。我们期待这项工作能激发未来在NVFP4模型训练与部署领域的进一步探索。

---

## 2. 大规模语言模型测试时计算扩展的艺术

- **原文链接**: http://arxiv.org/abs/2512.02008v1
- **发布日期**: 2025-12-01
- **摘要**:

测试时缩放（TTS）——即在推理过程中动态分配计算资源——是提升大语言模型（LLMs）推理能力的一个前景广阔的方向。然而，目前尚缺乏在相同条件下对主流TTS策略的系统性比较，且模型类型与问题难度对性能的影响仍不明确。为填补这些空白，我们开展了首次大规模TTS研究，涵盖使用八个开源LLM（参数量从70亿到2350亿）生成的超过三百亿个token，并覆盖四个推理数据集。我们观察到三个一致趋势：（1）不存在普遍最优的单一TTS策略；（2）推理模型在不同问题难度和推理轨迹长度上呈现差异化的轨迹质量模式，可分为短视野与长视野两类；（3）对于特定模型类型，最优TTS性能随计算预算增加呈单调提升。基于这些发现，我们提出了一套实用方案，结合问题难度、模型类型和计算预算来选择最佳TTS策略，为高效的推理时缩放提供了实践指南。

---

## 3. AlignSAE：概念对齐稀疏自编码器

- **原文链接**: http://arxiv.org/abs/2512.02004v1
- **发布日期**: 2025-12-01
- **摘要**:

大型语言模型（LLM）将事实性知识编码在难以检测或控制的隐藏参数空间中。虽然稀疏自编码器（SAE）能够将隐藏激活分解为更细粒度、可解释的特征，但它们往往难以可靠地将这些特征与人类定义的概念对齐，导致特征表示存在纠缠和分散的问题。为解决这一挑战，我们提出了AlignSAE方法，该方法通过"预训练-后训练"的课程设计，将SAE特征与既定本体进行对齐。在初始的无监督训练阶段后，我们采用有监督的后训练将特定概念绑定到专用潜在槽位，同时保留其余容量用于通用重构。这种分离机制创建了一个可解释的交互界面，使得特定关系能够在不受无关特征干扰的情况下被检测和控制。实验结果表明，AlignSAE通过定位单一语义对齐的槽位，能够实现精确的因果干预（例如可靠的"概念替换"）。

---

