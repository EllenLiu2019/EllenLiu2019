# 每日论文速递 - 2025-12-03

**更新日期**: 2025-12-03

---

## 1. 道德一致性管道：大型语言模型的持续伦理评估

- **原文链接**: http://arxiv.org/abs/2512.03026v1
- **发布日期**: 2025-12-02
- **摘要**:

大型语言模型（LLM）的快速发展和适应性凸显了道德一致性的必要性——即在多样化情境中保持伦理推理连贯性的能力。现有的对齐框架（旨在使模型行为与人类伦理及社会规范保持一致的结构化方法）通常依赖静态数据集和事后评估，难以揭示伦理推理在不同情境或时间尺度下的演变机制。本研究提出道德一致性管道（MoCoP），这是一种无需数据集、闭环式的框架，用于持续评估和解释LLM的道德稳定性。MoCoP在自持式架构中整合了三个支撑层：（i）词汇完整性分析，（ii）语义风险估计，以及（iii）基于推理的判断建模，该架构无需外部监督即可自主生成、评估和优化伦理场景。我们在GPT-4-Turbo和DeepSeek上的实验结果表明，MoCoP能有效捕捉纵向伦理行为，揭示伦理维度与毒性维度间的强负相关关系（相关系数rET = -0.81，p值小于0.001），且与响应延迟近乎无关（相关系数rEL约等于0）。这些发现表明，道德一致性与语言安全性往往作为模型行为稳定且可解释的特征出现，而非短期波动现象。此外，通过将伦理评估重构为动态、模型无关的道德内省形式，MoCoP为可扩展的持续审计提供了可复现的基础，并推动了自主AI系统中计算道德研究的发展。

---

## 2. LORE：面向搜索相关性的大规模生成模型

- **原文链接**: http://arxiv.org/abs/2512.03025v1
- **发布日期**: 2025-12-02
- **摘要**:

**成果**。我们提出了LORE，一个面向电商搜索场景的大规模生成模型相关性系统框架。经过三年部署与迭代，LORE在线上GoodRate指标上累计提升达+27%。本报告将分享其从数据、特征、训练、评估到部署全生命周期中积累的宝贵经验。**洞察**。现有研究虽尝试通过思维链（CoT）提升相关性，但常遭遇性能瓶颈。我们认为其根源在于将相关性视为单一任务，缺乏系统性解构。我们的核心洞见是：相关性由知识推理、多模态匹配与规则遵循三大能力构成。主张通过质化驱动的能力解构，是实现当前性能突破的关键。**贡献**。LORE为LLM相关性应用提供了完整技术蓝图，主要贡献包括：（1）提出两阶段训练范式，结合基于SFT的渐进式CoT合成与基于RL的人类偏好对齐；（2）构建综合性评测基准RAIR，针对性评估三大核心能力；（3）设计基于查询频次的分层部署策略，实现离线LLM能力向线上系统的高效迁移。LORE既可作为垂直领域的实用解决方案，也为其他领域提供了方法论参考。

---

## 3. 微调大型语言模型实现逻辑翻译：利用Lang2Logic减少幻觉现象

- **原文链接**: http://arxiv.org/abs/2512.02987v1
- **发布日期**: 2025-12-02
- **摘要**:

自然语言处理（NLP）领域的最新进展，特别是大语言模型（LLM）的发展，推动了无需人工干预的自然语言语句自动转换为形式逻辑的研究。这一进展使得自动化推理成为可能，并有助于软件系统中的调试、循环不变式发现以及规范遵循。然而，LLM产生的幻觉（即错误输出）问题构成了严峻挑战，尤其是在需要精确性的逻辑翻译任务中。本研究提出了一种创新框架：输入英语句子，将其转换为逻辑表达式，再转化为合取范式（CNF）以进行可满足性求解。该框架结合了自定义语法的经典NLP技术、符号计算库以及经过微调的语言模型，旨在减少幻觉现象。在早期实验中我们观察到，经过不同语法设置训练的微调模型能够有意识地修正原始模型产生的同类幻觉错误，从而实现了可靠的CNF生成。

---

