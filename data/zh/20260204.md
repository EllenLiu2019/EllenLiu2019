# 每日论文速递 - 2026-02-04

**更新日期**: 2026-02-04

---

## 1. 无奖励对齐：应对冲突目标

- **原文链接**: http://arxiv.org/abs/2602.02495v1
- **发布日期**: 2026-02-02
- **摘要**:

直接对齐方法正日益广泛地用于将大语言模型与人类偏好对齐。然而，许多现实世界的对齐问题涉及多个相互冲突的目标，此时简单聚合偏好可能导致训练不稳定和权衡失当。具体而言，加权损失方法可能无法找到同时改进所有目标的更新方向，而现有的多目标方法通常依赖显式奖励模型，这会引入额外复杂性并扭曲用户指定的偏好。本文的贡献主要体现在两个方面：首先，我们提出了面向冲突目标的免奖励对齐框架，该框架直接利用成对偏好数据，并通过一种新颖的冲突规避梯度下降剪裁变体来解决梯度冲突问题。我们提供了收敛到帕累托临界点的理论保证，这些临界点严格遵循用户指定的目标权重，并进一步证明在双目标场景中剪裁操作能严格提升收敛速率。其次，我们通过启发式方法改进该框架，并进行实验验证其在大语言模型对齐任务中的适用性。在多目标摘要生成和安全对齐任务上，通过对多个大语言模型系列（Qwen 3、Llama 3、Gemma 3）的定性与定量评估表明，相较于现有多目标对齐基线方法，我们提出的框架能持续实现更优的帕累托权衡。

---

## 2. RLAnything：在完全动态强化学习系统中锻造环境、策略与奖励模型

- **原文链接**: http://arxiv.org/abs/2602.02488v1
- **发布日期**: 2026-02-02
- **摘要**:

我们提出RLAnything框架，这是一种通过闭环优化动态构建环境、策略与奖励模型的强化学习系统，能够增强学习信号并强化适用于任意大语言模型或智能体场景的整体RL架构。具体而言，策略模型通过整合步进式信号与结果信号的反馈进行训练，而奖励模型则通过一致性反馈进行联合优化，这种优化反过来又能进一步提升策略训练效果。此外，我们基于理论驱动的自动环境适配机制，通过利用策略与奖励模型的批判性反馈，实现了从经验中学习，从而同步提升两者的训练效果。实证研究表明，每个新增组件都能持续改进整体系统性能：在OSWorld任务中，RLAnything将Qwen3-VL-8B-Thinking模型性能提升9.1%；在AlfWorld和LiveBench任务中，分别将Qwen2.5-7B-Instruct模型性能提升18.7%和11.9%。我们还发现，经过优化的奖励模型信号优于依赖人工标注的结果。代码已开源：https://github.com/Gen-Verse/Open-AgentRL

---

## 3. RE-TRAC：面向深度搜索代理的递归轨迹压缩技术

- **原文链接**: http://arxiv.org/abs/2602.02486v1
- **发布日期**: 2026-02-02
- **摘要**:

基于大语言模型的深度研究智能体主要建立在ReAct框架之上。这种线性设计难以回溯早期状态、无法拓展替代搜索方向，且在长上下文环境中缺乏全局感知能力，常导致局部最优、冗余探索和低效搜索。我们提出Re-TRAC框架，该智能体框架通过跨轨迹探索机制，在每条轨迹结束后生成结构化状态表征，用以总结证据、不确定性、失败案例及未来计划，并以此状态表征指导后续轨迹。这种设计实现了迭代反思与全局知情规划，将研究重构为渐进式过程。实验结果表明，在BrowseComp基准测试中，Re-TRAC使用前沿大语言模型时持续优于ReAct框架15-20%。针对较小模型，我们引入Re-TRAC感知监督微调方法，在同等规模下实现了最先进的性能。值得注意的是，Re-TRAC在多轮迭代中呈现出工具调用次数和令牌用量的单调递减趋势，表明其通过跨轨迹反思实现了渐进式定向探索，而非冗余搜索。

---

