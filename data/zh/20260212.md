# 每日论文速递 - 2026-02-12

**更新日期**: 2026-02-12

---

## 1. 量子审计：评估大语言模型在量子计算领域的推理极限

- **原文链接**: http://arxiv.org/abs/2602.10092v1
- **发布日期**: 2026-02-10
- **摘要**:

语言模型已成为量子计算教育与研究的实用工具，从技术论文总结到理论概念阐释，再到解答领域最新进展问题。虽然现有基准测试能评估量子代码生成与电路设计能力，但模型对量子计算概念的理解尚未得到系统化衡量。Quantum-Audit基准通过涵盖量子计算核心主题的2700道题目填补了这一空白。我们对26个主流机构的模型进行了评估，该基准包含1000道专家撰写题、1000道经专家验证的LLM论文提取题，另增设700道题目——其中350道开放式问题与350道包含错误前提的题目，用于测试模型能否纠正错误假设。人类参与者得分区间为23%至86%，专家平均正确率为74%。表现最佳的模型超越了专家平均水平，Claude Opus 4.5达到84%准确率，但顶尖模型在专家撰写题上的平均准确率较LLM生成题下降12个百分点。在高级主题上模型表现进一步下滑，安全类问题准确率降至73%。此外，模型常会接受并强化题目中隐含的错误前提而非识别它们，在这类关键推理任务中准确率低于66%。

---

## 2. 智能体世界模型：面向智能体强化学习的无限合成环境

- **原文链接**: http://arxiv.org/abs/2602.10090v1
- **发布日期**: 2026-02-10
- **摘要**:

大型语言模型（LLM）的最新进展使得自主智能体能够执行需要与工具和环境进行多轮交互的复杂任务。然而，由于缺乏多样且可靠的环境，此类智能体训练的规模化受到限制。本文提出**智能体世界模型（AWM）**——一种完全合成的环境生成流程。通过该流程，我们构建了涵盖日常场景的1000个环境，智能体可在其中与丰富的工具集（平均每个环境含35种工具）交互并获得高质量观测。值得注意的是，这些环境由代码驱动并以数据库为支撑，相比LLM模拟的环境能提供更可靠、一致的状态转换。此外，与从现实环境收集轨迹相比，它们能实现更高效的智能体交互。为验证该资源的有效性，我们对多轮工具使用智能体进行了大规模强化学习训练。得益于完全可执行的环境和可访问的数据库状态，我们还能设计可靠的奖励函数。在三个基准测试上的实验表明，仅在合成环境中训练（而非针对特定基准环境）能产生强大的分布外泛化能力。代码已开源：https://github.com/Snowflake-Labs/agent-world-model。

---

## 3. 提升科学图表分析能力的智能代理

- **原文链接**: http://arxiv.org/abs/2602.10081v1
- **发布日期**: 2026-02-10
- **摘要**:

在科学研究中，分析工作需准确解读复杂的多模态知识，整合不同来源的证据，并基于领域特定知识进行推理。然而，当前人工智能系统难以稳定展现此类能力。科学图表因其复杂多变的特性，结合异构结构与长上下文需求，构成了科学图表分析的根本障碍。为量化这些挑战，我们提出AnaBench——一个包含九大科学领域$63,178$个实例的大规模基准测试集，并沿七个复杂度维度进行系统分类。

为应对这些挑战，我们提出Anagent多智能体框架，通过四个专业智能体增强科学图表分析能力：规划器将任务分解为可执行的子任务，专家器通过定向工具执行获取任务特定信息，求解器综合信息生成连贯分析，评审器通过五维质量评估进行迭代优化。我们进一步开发模块化训练策略，结合监督微调与专业化强化学习，在保持高效协作的同时优化个体能力。

在170个子领域的综合评估中，Anagent实现了显著提升：无需训练的场景下性能提升达$\uparrow 13.43\%$，经微调后提升达$\uparrow 42.12\%$。研究同时揭示，面向任务的推理与上下文感知的问题解决能力是实现高质量科学图表分析的关键。项目主页：https://xhguo7.github.io/Anagent/。

---

