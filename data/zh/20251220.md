# 每日论文速递 - 2025-12-20

**更新日期**: 2025-12-20

---

## 1. 生成式对抗推理器：通过对抗性强化学习增强大语言模型推理能力

- **原文链接**: http://arxiv.org/abs/2512.16917v1
- **发布日期**: 2025-12-18
- **摘要**:

具备显式推理能力的大语言模型在数学推理方面表现出色，但仍存在过程性错误，例如计算错误、逻辑脆弱性以及表面合理但实际无效的推理步骤。本文提出生成对抗推理器——一种基于策略的联合训练框架，通过对抗性强化学习协同进化大语言模型推理器与大语言模型判别器，从而提升推理能力。该框架采用计算高效的审查机制，将每条推理链划分为逻辑完整且长度相近的片段，判别器通过简洁的结构化论证评估每个片段的严谨性。学习过程融合互补信号：大语言模型推理器因产生逻辑一致且能得出正确答案的步骤而获得奖励，判别器则通过准确检测错误或区分推理过程中的痕迹获得奖励。这种机制产生了密集、校准良好、基于策略的步骤级奖励，补充了稀疏的精确匹配信号，从而改进了信用分配、提高了样本效率，并增强了大语言模型的整体推理质量。在多种数学基准测试中，该方法相较于采用标准强化学习后训练的强基线模型均取得稳定提升。具体而言，在AIME24测试中，我们将DeepSeek-R1-Distill-Qwen-7B的得分从54.0提升至61.3（+7.3），将DeepSeek-R1-Distill-Llama-8B的得分从43.7提升至53.7（+10.0）。模块化判别器还能灵活支持奖励塑形，适用于教师蒸馏、偏好对齐及基于数学证明的推理等目标。

---

## 2. 建设性电路放大：通过针对性子网络更新提升大语言模型的数学推理能力

- **原文链接**: http://arxiv.org/abs/2512.16914v1
- **发布日期**: 2025-12-18
- **摘要**:

先前关于大语言模型内部工作机制的研究揭示了被称为"电路"的稀疏子网络，这些子网络负责执行特定任务。此外，研究表明通过微调提升模型性能往往源于对模型中现有电路的强化。综合这些发现，我们推测直接干预此类电路以实现精准、任务导向的更新具有可行性。基于此，我们提出了一种名为"建构式电路增强"的新方法，该方法通过分析模型推理轨迹识别关键标记，定位负责目标任务的模型组件，并仅更新这些特定组件。在数学推理任务上的应用表明，该方法仅需修改1.59%的模型组件，即可在多个模型上实现最高11.4%的准确率提升，同时根据MMLU、TriviaQA和TruthfulQA的评估，对其他能力的影响微乎其微。这些结果证明，通过选择性更新稀疏的模型组件集合，能够可靠地增强模型的定向能力。

---

## 3. 探索与利用：通过剪裁、熵与伪奖励重新审视RLVR

- **原文链接**: http://arxiv.org/abs/2512.16912v1
- **发布日期**: 2025-12-18
- **摘要**:

本文研究了具有可验证奖励的强化学习（RLVR）中的探索-利用权衡问题，该框架旨在提升大语言模型（LLMs）的推理能力。近期研究表明，RLVR可通过两种看似矛盾的机制激发LLMs强大的数学推理能力：一是伪奖励机制，即通过奖励与真实答案无关的结果来抑制模型对已知策略的过度利用；二是熵最小化机制，即推动模型输出更自信且确定的结果以抑制探索行为。这一矛盾动态凸显了一个令人困惑的现象：抑制利用与抑制探索均能提升推理性能，但调和这两种效应的内在原理仍不明确。我们聚焦于两个核心问题：（1）策略熵如何影响性能；（2）伪奖励是否通过裁剪偏差与模型污染的相互作用产生增益。实验结果表明，在伪奖励作用下，裁剪偏差会降低策略熵，促使模型输出更自信且确定的结果，而仅靠熵最小化并不足以带来性能提升。我们进一步提出奖励错配模型，以解释为何伪奖励能在非污染场景下提升性能。本研究阐明了伪奖励产生增益的内在机制，并为更高效的RLVR训练提供了理论依据。

---

